{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. High-Level Overview of the Transformer\n",
   "id": "626e7dfbbab9906c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "The attention mechanism can be summarized by:\n",
    "\n",
    "$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n",
    "$\n",
    "\n",
    "Where \\( Q \\), \\( K \\), \\( V \\) represent queries, keys, and values, each shaped (for each head) as:\n",
    "$ Q, K, V \\in \\mathbb{R}^{(\\text{seq\\_len}) \\times (d_k)} $\n",
    "\n",
    "and $( d_k = \\frac{d_\\text{model}}{n_\\text{heads}} )$.\n",
    "\n",
    "- **Multi-head** means we split the embedding dimension into $( n_\\text{heads} )$ parts, compute the above attention in parallel, then concatenate the outputs.\n",
    "\n",
    "#### Why Multi-Head?\n",
    "Instead of relying on a single attention distribution, multiple heads let the model attend to different positions (and different representation subspaces) at each layer.\n",
    "\n",
    "### Position-wise Feed-Forward Networks (FFN)\n",
    "\n",
    "After the self-attention sub-layer, each token is passed through a two-layer MLP:\n",
    "$$\n",
    "\\text{FFN}(\\mathbf{x}) = \\max(0,\\, \\mathbf{x} W_1 + b_1)\\, W_2 + b_2\n",
    "$$\n",
    "where:\n",
    "- $( W_1 \\in \\mathbb{R}^{d_\\text{model} \\times d_\\text{ff}} ), ( b_1 \\in \\mathbb{R}^{d_\\text{ff}} )$\n",
    "- $( W_2 \\in \\mathbb{R}^{d_\\text{ff} \\times d_\\text{model}} ), ( b_2 \\in \\mathbb{R}^{d_\\text{model}} \\$\n",
    "\n",
    "\n",
    "###Positional Encoding\n",
    "\n",
    "Since the Transformer is permutation-invariant (due to attention alone), we need to inject sequence-order information into the embeddings.\n",
    "\n",
    "- **Sinusoidal positional encodings**:\n",
    "  $$\n",
    "  PE_{(pos, 2i)} = \\sin\\Bigl(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}\\Bigr),\n",
    "  \\quad\n",
    "  PE_{(pos, 2i+1)} = \\cos\\Bigl(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}}\\Bigr)\n",
    "  $$\n",
    "- **Learned positional embeddings**: A trainable embedding table for positions.\n",
    "- **Relative positional encoding**: Focuses on the distance between positions.\n",
    "- **Rotary embeddings** (RoPE): Introduced in [RoFormer](https://arxiv.org/abs/2104.09864), helpful for extending context length.\n",
    "- **Hybrid**: Combining sinusoidal or rotary with learned embeddings.\n",
    "\n",
    "---\n",
    "\n",
    "### Residual Connections & Layer Normalization\n",
    "\n",
    "Each sub-layer (Self-Attention or FFN) is wrapped with:\n",
    "```\n",
    "x -> SubLayer(x) -> x + SubLayer(x) -> LayerNorm\n",
    "```\n",
    "This helps stabilize training and reduce gradient vanishing/exploding in deep networks.\n",
    "\n",
    "\n"
   ],
   "id": "4b4a6e6f52446b83"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Coding Transformer\n",
    "\n",
    "We start wth  three training weight matrices $W_q$, $W_k$, and $W_v$ which will be used to project the embedded input tokens, $x^{(i)}$, into query, key, and value vectors via matrix multiplication:\n",
    "\n",
    "  - Query vector: $q^{(i)} = W_q \\,x^{(i)}$\n",
    "  - Key vector: $k^{(i)} = W_k \\,x^{(i)}$\n",
    "  - Value vector: $v^{(i)} = W_v \\,x^{(i)}$"
   ],
   "id": "8b3c7605417fccb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn import functional as F\n"
   ],
   "id": "ac6e9b7a67cf60a9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "batch = torch.stack((inputs, inputs), dim=0)"
   ],
   "id": "692e42227c569a73"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "x_2 = inputs[1] # second input element\n",
    "d_in = inputs.shape[1] # the input embedding size, d=3\n",
    "d_out = 2 # the output embedding size, d=2"
   ],
   "id": "2185fdf4a0b9f232"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "- d_in: dim of the i/p features for each token.\n",
    "- d_out: dim of the  o/p features for each token.\n",
    "- context_length: Max number of tokens in the input sequence\n",
    "- dropout: Dropout probability for regularization in the attention weights.\n",
    "- qkv_bias:\n",
    "- Three separate learnable linear transformations are defined to project the input embeddings into  Q,K, and V\n",
    "- Dropout is applied to the attention weights to prevent overfitting.\n",
    "- Causal mask,  upper triangular mask with ones above the diagonal and zeros below.\n",
    "\n"
   ],
   "id": "366d243c70586be9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class CausalAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length,\n",
    "                 dropout, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.d_out = d_out\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.dropout = nn.Dropout(dropout) # New\n",
    "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape # New batch dimension b\n",
    "        keys = self.W_key(x)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        attn_scores = queries @ keys.transpose(1, 2) # Changed transpose\n",
    "        attn_scores.masked_fill_(  # New, _ ops are in-place\n",
    "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
    "        attn_weights = torch.softmax(\n",
    "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
    "        )\n",
    "        attn_weights = self.dropout(attn_weights) # New\n",
    "\n",
    "        context_vec = attn_weights @ values\n",
    "        return context_vec\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1]\n",
    "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
    "\n",
    "context_vecs = ca(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ],
   "id": "af68878830993d0e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Stacking multiple single-head attention layers\n",
    " We simply stack multiple single-head attention modules to obtain a multi-head attention module: The main idea behind multi-head attention is to run the attention mechanism multiple times (in parallel) with different, learned linear projections. This allows the model to jointly attend to information from different representation subspaces at different positions.\n",
    "\n"
   ],
   "id": "7b2ab15439579229"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
    "             for _ in range(num_heads)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([head(x) for head in self.heads], dim=-1)\n",
    "\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "context_length = batch.shape[1] # This is the number of tokens\n",
    "d_in, d_out = 3, 2\n",
    "mha = MultiHeadAttentionWrapper(\n",
    "    d_in, d_out, context_length, 0.0, num_heads=2\n",
    ")\n",
    "\n",
    "context_vecs = mha(batch)\n",
    "\n",
    "print(context_vecs)\n",
    "print(\"context_vecs.shape:\", context_vecs.shape)"
   ],
   "id": "996be7ef947c9339"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a3b5a8da949e2db3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Using PyTorch’s `nn.Transformer` Module\n",
    "\n",
    "---"
   ],
   "id": "8f8bdf0bb25579e6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "transformer_model = nn.Transformer(\n",
    "    d_model=512,\n",
    "    nhead=8,\n",
    "    num_encoder_layers=6,\n",
    "    num_decoder_layers=6,\n",
    "    dim_feedforward=2048\n",
    ")\n",
    "\n",
    "# Example shapes:\n",
    "src = torch.randn(10, 32, 512)  # (src_seq_len, batch_size, d_model)\n",
    "tgt = torch.randn(20, 32, 512)  # (tgt_seq_len, batch_size, d_model)\n",
    "\n",
    "out = transformer_model(src, tgt)  # => (tgt_seq_len, batch_size, d_model)"
   ],
   "id": "896a2706719b3c86"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "> **Note on Shapes**: PyTorch’s built-in `nn.Transformer` expects input shape `(sequence_length, batch_size, d_model)`\n",
    "\n",
    "Notes\n",
    "1. **Attention Mechanisms**:\n",
    "   - Understand *scaled dot-product* in detail.\n",
    "   - Distinguish self-attention from cross-attention (encoder-decoder attention).\n",
    "\n",
    "2. **Positional Encoding Innovations**:\n",
    "   - Sinusoidal vs. learned vs. relative vs. rotary.\n",
    "   - The trade-offs in practice (e.g., learned embeddings can be more flexible, but sinusoidal is simpler, rotary better for extrapolation, etc.).\n",
    "\n",
    "3. **Layer Normalization and Residuals**:\n",
    "   - Vital for stable deep learning, especially in large models.\n",
    "\n",
    "4. **Model Scaling**:\n",
    "   - Modern Transformers can have dozens of layers and thousands of hidden dimensions.\n",
    "   - Expert knowledge requires understanding of memory and computational constraints, as well as *mixed precision* training, distributed training, etc.\n",
    "\n",
    "5. **Masking**:\n",
    "   - **Padding Mask**: Avoid attending to `<pad>` tokens.\n",
    "   - **Causal Mask**: Prevent the decoder from attending to future tokens.\n",
    "   - **Cross-Attention** Mask: If needed for ignoring padded portions of the encoder output.\n",
    "\n",
    "6. **Implementation Details**:\n",
    "   - Efficiency: GPU memory usage, chunked attention, flash attention, etc.\n",
    "   - GPU/TPU/HPU performance tuning: half-precision, gradient checkpointing for large models.\n",
    "   - Large-scale training frameworks (e.g., [DeepSpeed](https://github.com/microsoft/DeepSpeed), [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)).\n",
    "\n",
    "Additional Notes\n",
    "1. **Original Paper**: [*Attention Is All You Need*](https://arxiv.org/abs/1706.03762) by Vaswani et al.\n",
    "2. **Hugging Face Transformers**: For pre-trained models (BERT, GPT, T5, etc.), check out [Hugging Face’s Transformers library](https://github.com/huggingface/transformers).\n",
    "3. **Performance Optimizations**:\n",
    "   - *Flash Attention*: [Paper](https://arxiv.org/abs/2205.14135), [Open-source implementation](https://github.com/HazyResearch/flash-attention).\n",
    "   - *Zero Redundancy Optimizer (ZeRO)* in [DeepSpeed](https://www.deepspeed.ai/).\n",
    "\n",
    "4. **Advanced Topics**:\n",
    "   - Techniques like *Adapter Layers* for efficient fine-tuning.\n",
    "   - *Prompt tuning* or *prefix tuning*.\n",
    "   - *Sparse Attention* for long sequences, e.g., Longformer, Big Bird.\n",
    "   - *Retrieval-Augmented Transformers* for knowledge grounding.\n",
    "\n",
    "---\n"
   ],
   "id": "e040a855af7a285c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
