{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thedatasense/llm-healthcare/blob/main/MIMIC_GPT_Evaluation_C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4b6ArvcGvFf",
        "outputId": "19dd11a0-f734-41d9-a9c7-4d6accfb4600"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/3.0 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/3.0 MB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q  sqlalchemy cockroachdb pandas psycopg2-binary matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPISEELWNK4h",
        "outputId": "7bce9da7-8ff3-46a3-b5a4-9a80e04fa6cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  2728    0  2728    0     0  13448      0 --:--:-- --:--:-- --:--:-- 13438\r100  2728    0  2728    0     0  13442      0 --:--:-- --:--:-- --:--:-- 13438\n"
          ]
        }
      ],
      "source": [
        "!curl --create-dirs -o $HOME/.postgresql/root.crt 'https://cockroachlabs.cloud/clusters/5bbbe91d-b65e-410e-a783-597c93f501f6/cert'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mYj_N4onKe2h"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import clear_output\n",
        "from sqlalchemy.engine import create_engine\n",
        "from google.colab import userdata\n",
        "engine = create_engine(userdata.get('DB_URL'))\n",
        "#from datasets import load_dataset\n",
        "from openai import OpenAI\n",
        "import io\n",
        "import base64\n",
        "import random\n",
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "#from transformers import AutoProcessor,Qwen2_5_VLForConditionalGeneration\n",
        "#from qwen_vl_utils import process_vision_info\n",
        "import os\n",
        "import pandas as pd\n",
        "from sqlalchemy.engine import create_engine\n",
        "from transformers import AutoProcessor, BitsAndBytesConfig\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T3xF40KTLRKi",
        "outputId": "b15f69af-2b3b-4399-ad55-0fd6eabc6c75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynYZ3dOl21xC",
        "outputId": "67eed406-221b-4e47-c1e8-422867e7a9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'llm-healthcare'...\n",
            "remote: Enumerating objects: 1339, done.\u001b[K\n",
            "remote: Counting objects: 100% (171/171), done.\u001b[K\n",
            "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
            "remote: Total 1339 (delta 111), reused 106 (delta 53), pack-reused 1168 (from 2)\u001b[K\n",
            "Receiving objects: 100% (1339/1339), 34.88 MiB | 21.31 MiB/s, done.\n",
            "Resolving deltas: 100% (275/275), done.\n",
            "Updating files: 100% (1385/1385), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/thedatasense/llm-healthcare.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zftRNEw1stLO"
      },
      "outputs": [],
      "source": [
        "def insert_model_response(engine, uid,question_id,question, question_category, actual_answer, model_name, model_answer, image_link):\n",
        "    from sqlalchemy import text\n",
        "    with engine.connect() as conn:\n",
        "        trans = conn.begin()\n",
        "        try:\n",
        "            conn.execute(text(\"\"\"\n",
        "                INSERT INTO model_responses_r2\n",
        "                (uid,question_id,question, question_category, actual_answer, model_name, model_answer, image_link)\n",
        "                VALUES (:uid,:question_id,:question, :question_category, :actual_answer, :model_name, :model_answer, :image_link)\n",
        "            \"\"\"), {\n",
        "                \"uid\": uid,\n",
        "                \"question_id\": question_id,\n",
        "                \"question\": question,\n",
        "                \"question_category\": question_category,\n",
        "                \"actual_answer\": actual_answer,\n",
        "                \"model_name\": model_name,\n",
        "                \"model_answer\": model_answer,\n",
        "                \"image_link\": image_link\n",
        "            })\n",
        "            trans.commit()  # Commit the transaction\n",
        "        except Exception as e:\n",
        "            trans.rollback()\n",
        "            raise e\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4-EulRd2Mo1"
      },
      "outputs": [],
      "source": [
        "def check_duplicate(engine,uid,question_id,question, question_category, model_name,image_link):\n",
        "    query = text(\"\"\"\n",
        "        SELECT 1 FROM model_responses_r2\n",
        "        WHERE\n",
        "        uid = :uid\n",
        "        AND question_id = :question_id and\n",
        "        question = :question\n",
        "          AND question_category = :question_category\n",
        "          AND model_name = :model_name\n",
        "          AND image_link = :image_link\n",
        "        LIMIT 1\n",
        "    \"\"\")\n",
        "    with engine.connect() as conn:\n",
        "        result = conn.execute(query, {\n",
        "            \"uid\": uid,\n",
        "            \"question_id\": question_id,\n",
        "            \"question\": question,\n",
        "            \"question_category\": question_category,\n",
        "            \"model_name\": model_name,\n",
        "            \"image_link\": image_link\n",
        "        }).fetchone()\n",
        "    return result is not None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FIav8OWs_6I"
      },
      "outputs": [],
      "source": [
        "def fetch_generation_data(engine):\n",
        "    import pandas as pd\n",
        "    import re\n",
        "    from sqlalchemy import text\n",
        "    from sqlalchemy.dialects.postgresql.base import PGDialect\n",
        "    def fake_get_server_version_info(self, connection):\n",
        "        version_str = connection.execute(text(\"SELECT version()\")).scalar()\n",
        "        match = re.search(r'v(\\d+)\\.(\\d+)\\.(\\d+)', version_str)\n",
        "        if match:\n",
        "            return tuple(map(int, match.groups()))\n",
        "        return (13, 0, 0)\n",
        "    PGDialect._get_server_version_info = fake_get_server_version_info\n",
        "    query = f\"SELECT id,question_id,condition as question_type, text as question,answer as ground_truth,image from mimic_all_qns; \"\n",
        "    return pd.read_sql(query, con=engine)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rISoY37mfpIf"
      },
      "outputs": [],
      "source": [
        "oai_key=userdata.get('OPENAI_SAIL')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3h7GpPmrwUKT"
      },
      "outputs": [],
      "source": [
        "def encode_image_stream(image_path):\n",
        "    try:\n",
        "        if not os.path.exists(image_path):\n",
        "            error_file_list.add(image_path)\n",
        "            return None\n",
        "\n",
        "        with open(image_path, \"rb\") as img_file:\n",
        "            return base64.b64encode(img_file.read()).decode(\"utf-8\")\n",
        "    except Exception as e:\n",
        "        error_file_list.add(image_path)\n",
        "        return None\n",
        "\n",
        "def generate_gpt_response(prompt_text, image_link):\n",
        "    client = OpenAI(api_key=oai_key)\n",
        "    base64_image = encode_image_stream(image_link)\n",
        "\n",
        "    if base64_image is None:\n",
        "        return f\"Unable to process image: {image_link}\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": (\n",
        "                    \"You are an expert medical professional. \"\n",
        "                    \"When responding, provide a concise explanation of the image findings. \"\n",
        "                    \"For example, if asked about abnormalities, answer briefly with terms like 'atelectasis, lung opacity'.\"\n",
        "                )\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"text\",\n",
        "                        \"text\": prompt_text,\n",
        "                    },\n",
        "                    {\n",
        "                        \"type\": \"image_url\",\n",
        "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}\n",
        "                    },\n",
        "                ],\n",
        "            },\n",
        "        ],\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3S9IS2o18wI0"
      },
      "outputs": [],
      "source": [
        "source_folder='/content/drive/MyDrive/Health_Data/MIMIC_JPG_AVL/mimic-cxr-jpg/2.1.0/files/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QonkhLqzV3Gu"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def print_a_sample(df,prompt_prefix,source_folder=source_folder,width=250):\n",
        "    # Get a random row as a Series\n",
        "    row = df.sample(n=1).iloc[0]  # Add .iloc[0] to get the row as a Series\n",
        "\n",
        "    # Now these will return scalar values\n",
        "    question = row[\"question\"]\n",
        "    actual_answer = row[\"ground_truth\"]\n",
        "    image_link = source_folder + row[\"image\"]\n",
        "\n",
        "    print(f\"{question}\")\n",
        "    print(f\"GT: {actual_answer}\")\n",
        "    print(f\"Image: {image_link}\")\n",
        "\n",
        "\n",
        "    # Uncommented to display the image\n",
        "    display(Image(filename=image_link, width=width))\n",
        "    generated_answer = generate_gpt_response(prompt_prefix + \" \" + row[\"question_type\"] + \":\" + row[\"question\"] + prompt_prefix, image_link)\n",
        "    print(f\"gpt-4o : {generated_answer}\")\n",
        "    print('--------------------------------')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LY98s-3su7dL"
      },
      "outputs": [],
      "source": [
        "from sqlalchemy import text\n",
        "import time\n",
        "error_file_list = set()\n",
        "model_id = \"gpt-4o\"\n",
        "\n",
        "for index, row in fetch_generation_data(engine).iterrows():\n",
        "    uid = row[\"id\"]\n",
        "    question_id = row[\"question_id\"]\n",
        "    question_category = row[\"question_type\"]\n",
        "    question = row[\"question\"]\n",
        "    actual_answer = row[\"ground_truth\"]\n",
        "    image_link = source_folder + row[\"image\"]\n",
        "\n",
        "    if check_duplicate(engine, uid, str(question_id), question, question_category, model_id, image_link):\n",
        "        clear_output(wait=True)\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        generated_answer = generate_gpt_response(question, image_link)\n",
        "\n",
        "        if not generated_answer.startswith(\"Unable to process image\"):\n",
        "            insert_model_response(\n",
        "                engine, uid, question_id, question, question_category,\n",
        "                actual_answer, model_id, generated_answer, image_link\n",
        "            )\n",
        "    except Exception as e:\n",
        "        error_file_list.add(image_link)\n",
        "        continue\n",
        "\n",
        "    time.sleep(5)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}